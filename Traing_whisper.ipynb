{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinbluechip/whisper-Llama/blob/main/Traing_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWoYeD7LxuF-",
        "outputId": "76bb4525-ca51-41e8-bf5c-b064d7049b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '@'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting jsonargparse[signatures]\n",
            "  Downloading jsonargparse-4.31.0-py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]) (6.0.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures])\n",
            "  Downloading typeshed_client-2.6.0-py3-none-any.whl (623 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m623.3/623.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]) (6.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]) (4.12.2)\n",
            "Installing collected packages: typeshed-client, jsonargparse\n",
            "Successfully installed jsonargparse-4.31.0 typeshed-client-2.6.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Collecting zstandard\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard\n",
            "Successfully installed zstandard-0.23.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (10.1.0)\n",
            "Collecting tiktoken==0.3.3\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ],
      "source": [
        "# from Lit llama  : https://github.com/Lightning-AI/lit-llama/blob/main/requirements.txt\n",
        "\n",
        "!pip install torch>=2.0.0\n",
        "!pip install lightning @ git+https://github.com/Lightning-AI/lightning@master\n",
        "!pip install sentencepiece\n",
        "!pip install tqdm  # convert_checkpoint.py\n",
        "!pip install numpy  # train.py dataset memmap\n",
        "!pip install jsonargparse[signatures]  # generate.py, convert_checkpoint.py CLI\n",
        "!pip install bitsandbytes  # quantization.py\n",
        "!pip install datasets  # evaluate.py\n",
        "!pip install zstandard  # prepare_redpajama.\n",
        "\n",
        "# from Whisper  : https://github.com/openai/whisper/blob/main/requirements.txt\n",
        "\n",
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install more-itertools\n",
        "!pip install tiktoken==0.3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone relevant repo and model weights\n"
      ],
      "metadata": {
        "id": "Yxs2SQdivHfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/whisper Lalama testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L0K40PByPbw",
        "outputId": "b9b655a9-73ad-40f7-a154-9a9c12b7fd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/whisper Lalama testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/Srijith-rkr/Whispering-LLaMA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZCt5Q_E1WWG",
        "outputId": "90ac33eb-3750-453e-a1f0-f2be5967682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Whispering-LLaMA'...\n",
            "remote: Enumerating objects: 242, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 242 (delta 44), reused 37 (delta 37), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (242/242), 10.74 MiB | 9.70 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/whisper Lalama testing/weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7-k2MEX1bzR",
        "outputId": "a76a6bcb-6532-4a93-a088-d1ed615ca34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/whisper Lalama testing/weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the weights and rename the folder as weights\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/Srijith-rkr/Whispering-LLaMA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF1ggTkz1pcH",
        "outputId": "861ce731-9943-4d59-e4b1-b63b5fb7bfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Whispering-LLaMA'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 26 (delta 6), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (26/26), 3.40 KiB | 10.00 KiB/s, done.\n",
            "Filtering content: 100% (4/4), 1.10 GiB | 703.00 KiB/s, done.\n",
            "fatal: cannot exec '/content/drive/MyDrive/whisper Lalama testing/weights/Whispering-LLaMA/.git/hooks/post-checkout': Permission denied\n",
            "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
            "\talpaca_b.pth\n",
            "\talpaca_c.pth\n",
            "\talpaca_a.pth\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merch the all weights into one alpaca"
      ],
      "metadata": {
        "id": "9Z5jm1Q0vPKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# To merge the Alpaca weigts from the HuggingFace repo we just cloned\n",
        "\n",
        "a = torch.load('/content/drive/MyDrive/whisper Lalama testing/weights/Whispering-LLaMA/alpaca_a.pth')\n",
        "b = torch.load('/content/drive/MyDrive/whisper Lalama testing/weights/Whispering-LLaMA/alpaca_b.pth')\n",
        "c = torch.load('/content/drive/MyDrive/whisper Lalama testing/weights/Whispering-LLaMA/alpaca_c.pth')\n",
        "\n",
        "# merging\n",
        "alpaca_checkpoint = a|b|c\n",
        "\n",
        "# saving\n",
        "torch.save(alpaca_checkpoint,'/content/drive/MyDrive/whisper Lalama testing/weights/alpaca.pth')"
      ],
      "metadata": {
        "id": "NGhJdGZ81tjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "J40LOJ4fvmmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This notebook uses https://github.com/openai/whisper with edits to the whisper_openAI/decoding.py to generate multiple hypothesis\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "IPH-q3E1_KsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To print nuber of datapoints per category in the gigaspeech dataset form Hugging Face\n",
        "\n",
        "sdata = load_dataset(\"speechcolab/gigaspeech\", \"s\", use_auth_token='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', cache_dir = '/ibex/user/radhaks/LLMs/LLaMA_7B/LLAMA_EMNLP_DeepSpeed/dataset/gigaspeech')\n",
        "\n",
        "scs = []\n",
        "for i in tqdm.tqdm(sdata['train'])    :\n",
        "    scs.append(i['category'])\n",
        "\n",
        "scategories = set(scs.copy())\n",
        "\n",
        "sdic = {}\n",
        "for c in scategories:\n",
        "    sdic[c]=0\n",
        "for c in scs:\n",
        "    sdic[c] = sdic[c]+1\n",
        "print(sdic)\n",
        "\n",
        "index = [\"People and Blogs\", \"Business\", \"Nonprofits and Activism\", \"Crime\", \"History\", \"Pets and Animals\", \"News and Politics\", \"Travel and Events\", \"Kids and Family\", \"Leisure\", \"N/A\", \"Comedy\", \"News and Politics\", \"Sports\", \"Arts\", \"Science and Technology\", \"Autos and Vehicles\", \"Science and Technology\", \"People and Blogs\", \"Music\", \"Society and Culture\", \"Education\", \"Howto and Style\", \"Film and Animation\", \"Gaming\", \"Entertainment\", \"Travel and Events\", \"Health and Fitness\", \"audiobook\"]\n",
        "for i in sdic.keys():\n",
        "    no = sdic[i]\n",
        "    cls = index[i]\n",
        "    print(f\"{i} {cls}: {no}\")"
      ],
      "metadata": {
        "id": "No0pOn3D-3H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting the categories you want\n",
        "cats_I_want = [0,2,3,11,12,14,15,17,18,21,22,23,24,25]\n",
        "class_dict ={}\n",
        "for i in cats_I_want:\n",
        "    class_dict[i]=[]\n",
        "for i in tqdm.tqdm(sdata['train'])    :\n",
        "    if i['category'] in cats_I_want:\n",
        "        class_dict[i['category']].append(i)"
      ],
      "metadata": {
        "id": "66J_9_se_Mhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict[25]"
      ],
      "metadata": {
        "id": "BclX9Ccg_PGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# moving to the whisper folder ; make sure you have the whisper environment on\n",
        "%cd ..\n",
        "import numpy\n",
        "# Renamed the Whisepr repo (https://github.com/openai/whisper) with the changed decoding.py file as whisper_openAI\n",
        "import whisper_openAI.whisper as whisper\n",
        "import torch\n",
        "import tqdm\n",
        "model ,_ = whisper.load_model(\"tiny\") # you can change the whisper model here to largev2 or large to swap the  model."
      ],
      "metadata": {
        "id": "X2hpquFb_ZY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_json=[]\n",
        "k = 17 # choosing category 17 in the gigaspeech dataset\n",
        "for i in tqdm.tqdm(class_dict[k]):\n",
        "\n",
        "    audio =  i['audio']['array'].astype(numpy.single)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    # Storing meta-data for the final .pt file\n",
        "    ground_truth = i['text'].replace(' <COMMA>',',').replace(' <PERIOD>','.').replace(' <QUESTIONMARK>','?').replace(' <EXCLAMATIONPOINT>','!').lower()\n",
        "    source = i['source']\n",
        "    cat = i['category']\n",
        "    time = i['end_time'] - i['begin_time']\n",
        "    path_to_file = i['audio']['path']\n",
        "\n",
        "    # As mentioned in the paper, we choose a random temperature\n",
        "    random_temprature = numpy.random.randint(70,81)/100\n",
        "    options = whisper.DecodingOptions(fp16 = True, without_timestamps = True, temperature=random_temprature,best_of=200)\n",
        "    # The below function is the only one that has been modified from the original Whisper repository (https://github.com/openai/whisper)\n",
        "    # The ouputs are unique sentences,\n",
        "    result,_ = whisper.decode(model, mel, options)\n",
        "    result = list(result)\n",
        "\n",
        "    # This block runs inference with an higher temperatur if the no of samples generated is less that 10 (Increasing the temperature, increases the number of unique sampels)\n",
        "    if len(result)<=10:\n",
        "       # print(f'got only {len(result)} with temp {random_temprature} going again')\n",
        "        if random_temprature< 0.75:\n",
        "            random_temprature = random_temprature + 0.2\n",
        "        else:\n",
        "            random_temprature = random_temprature + 0.1\n",
        "        options = whisper.DecodingOptions(fp16 = True, without_timestamps = True, temperature=random_temprature,best_of=200)\n",
        "        result,_ = whisper.decode(model, mel, options)\n",
        "        result = list(result)\n",
        "\n",
        "       # print(f'got {len(result)} now')\n",
        "    # saving the hypothesis as \"inference\" and other meta data\n",
        "    to_json.append( {i['segment_id']:{ 'temp': random_temprature,'path':path_to_file ,'ground_truth':ground_truth, 'inference' :result, 'source':source, 'category':cat, 'time':time, 'path':path_to_file}} )\n",
        "# saving the inference as json file\n",
        "import json\n",
        "save_path=f'/ibex/user/radhaks/LLMs/LLaMA_7B/LLAMA_EMNLP_DeepSpeed/dataset/inferences/gs_inferences/{str(k)}{index[k]}.json'\n",
        "with open(save_path, \"w\") as file:\n",
        "    json.dump(to_json,file,indent=4)"
      ],
      "metadata": {
        "id": "4LBYbWgC_fe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training/WL-M.py to train the best our best model on dataset"
      ],
      "metadata": {
        "id": "F5xP03zJvquf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import argparse\n",
        "\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# support running without installing as a package\n",
        "wd = Path(__file__).parent.parent.resolve() # does not work as jupyter notebook\n",
        "sys.path.append(str(wd))\n",
        "\n",
        "import whisper_openAI.whisper as whisper\n",
        "from lit_llama.WL_M import LLaMA, LLaMAConfig, mark_only_adapter_as_trainable, adapter_state_from_state_dict\n",
        "from lit_llama.tokenizer import Tokenizer\n",
        "#from scripts.prepare_alpaca import generate_prompt\n",
        "from lightning.fabric.strategies import DeepSpeedStrategy\n",
        "from generate.generate_for_WL import generate\n",
        "\n",
        "#cli setup\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lr', type=float, default=1e-3,help='learning rate for the model (default: 1e-3)')\n",
        "parser.add_argument('--d', type=int, default=1,help='No of GPUs (default: 1)')\n",
        "parser.add_argument('--pretrained_path', type=str,default= 'model/Alpaca_PY/lit-llama.pth',help='Path to Alpaca checkpoint')\n",
        "parser.add_argument('--tokenizer_path', type=str,help='Path to LLaMA tokenizer')\n",
        "parser.add_argument('--data_path', type=str,help='Path to data')\n",
        "\n",
        "args = parser.parse_args()\n",
        "learning_rate = args.lr\n",
        "pretrained_path = args.pretrained_path\n",
        "tokenizer_path = args.tokenizer_path\n",
        "data_path = args.data_path\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 25\n",
        "weight_decay = 0.02\n",
        "\n",
        "# Batch and device configuration\n",
        "devices = args.d\n",
        "batch_size = 32 / devices\n",
        "micro_batch_size = 4\n",
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "\n",
        "train_path = f'{data_path}_train.pt'\n",
        "val_path = f'{data_path}_test.pt'\n",
        "\n",
        "train_data = torch.load(train_path,map_location=torch.device('cpu'))\n",
        "val_data   = torch.load(val_path,map_location=torch.device('cpu'))\n",
        "\n",
        "train_data_len = len(train_data)\n",
        "val_data_len = len(val_data)\n",
        "\n",
        "print('loaded test data')\n",
        "\n",
        "epoch_size = train_data_len // micro_batch_size // devices\n",
        "max_iters = num_epochs * epoch_size\n",
        "eval_iters = val_data_len // micro_batch_size  // devices\n",
        "warmup_steps = epoch_size * 0 // devices\n",
        "\n",
        "# Context configuration\n",
        "max_seq_length = 2048\n",
        "max_input_length = 1000\n",
        "\n",
        "# Checkpointing configuration\n",
        "\n",
        "save_interval = epoch_size # save every epoch\n",
        "log_interval = 1\n",
        "run_name = f'WL_M_{learning_rate}'\n",
        "out_dir: str = 'runs/'+run_name\n",
        "\n",
        "# wandb configuration\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"GigaCat\",\n",
        "    name=run_name,\n",
        "    group=run_name,\n",
        "    config={\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"batch_size\": (batch_size*devices),\n",
        "    \"micro_batch_size\":micro_batch_size,\n",
        "    \"dataset\":'gigaspeech',\n",
        "    'devices':devices,\n",
        "    'max_input_length':max_input_length,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Use if needed, we use DDP strategy with the current implementation on 2x A100s\n",
        "ds_config = {\n",
        "    \"train_micro_batch_size_per_gpu\": micro_batch_size,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"zero_optimization\": {\"stage\": 2},\n",
        "}\n",
        "\n",
        "def main():\n",
        "    fabric = L.Fabric(\n",
        "        accelerator=\"cuda\",\n",
        "        devices=devices,\n",
        "        strategy= \"ddp\"  if devices > 1 else \"auto\" ,\n",
        "    # If the model is not fittng replace DDP with DeepSpeed strategy (DeepSpeedStrategy(config=ds_config) if devices > 1 else \"auto\")\n",
        "        precision=\"bf16-true\",\n",
        "    )\n",
        "    fabric.launch()\n",
        "    fabric.seed_everything(1337 + fabric.global_rank)\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    config = LLaMAConfig(block_size=max_seq_length)\n",
        "\n",
        "\n",
        "    if not os.path.isfile(pretrained_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Can't find the pretrained weights at {pretrained_path}.\"\n",
        "            \" Please follow the instructions in the README to download them.\"\n",
        "        )\n",
        "    checkpoint = torch.load(pretrained_path)\n",
        "    print('loaded LLaMA checkpoint')\n",
        "    with fabric.init_module():\n",
        "        model = LLaMA(config)\n",
        "\n",
        "    # Adding Cross Attention (K,V) weigths from the Whisper Decoder to Alpaca State_dict\n",
        "    (_, w_ck_pt) = whisper.load_model(\"large-v2\",device='cpu')\n",
        "    print('loaded Whisper checkpoint')\n",
        "    for n, p in model.named_parameters():\n",
        "        if 'whisper' in n :\n",
        "            #transformer.h.2.attn.whisper_value.weight\n",
        "            layer = n.split('.')[2]\n",
        "            suffix = n.split('.')[-1]\n",
        "            kv = n.split('.')[4].split('_')[-1]\n",
        "            #decoder.blocks.3.cross_attn.key.weight\n",
        "            w_key = f'decoder.blocks.{layer}.cross_attn.{kv}.{suffix}'\n",
        "            checkpoint[n] = w_ck_pt['model_state_dict'][w_key].cpu()\n",
        "\n",
        "\n",
        "\n",
        "    with fabric.init_module():\n",
        "         # strict=False because missing keys due to adapter weights not containted in state dict\n",
        "        model.load_state_dict(checkpoint, strict=False)\n",
        "    print('loaded LAMMA model')\n",
        "    mark_only_adapter_as_trainable(model)\n",
        "\n",
        "    num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    print(f\"Number of trainable parameters: {num_params/1e6}M\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    model, optimizer = fabric.setup(model, optimizer)\n",
        "    train(fabric, model, optimizer, train_data, val_data, out_dir)\n",
        "    wandb.finish()\n",
        "\n",
        "    # Save the final checkpoint at the end of training\n",
        "    save_model_checkpoint(fabric, model, os.path.join(out_dir, \"lit-llama-adapter-finetuned.pth\"))\n",
        "\n",
        "\n",
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    train_data: np.ndarray,\n",
        "    val_data: np.ndarray,\n",
        "    out_dir: str,\n",
        ") -> None:\n",
        "    \"\"\"The training loop.\n",
        "\n",
        "    Loosely based on the nanoGPT implementation: https://github.com/karpathy/nanoGPT.\n",
        "    \"\"\"\n",
        "    step_count = 0 # gets updated each time you compleate a batch aka each time you take a step\n",
        "\n",
        "    for iter_num in range(max_iters):\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        input_ids, targets, audio_features = get_batch(fabric, model, train_data)\n",
        "        logits = model(input_ids, audio_features = audio_features)\n",
        "        loss = loss_fn(logits, targets)\n",
        "        with fabric.no_backward_sync(model, enabled=((iter_num + 1) % gradient_accumulation_steps != 0)): # Skip gradient synchronization during backward to avoid redundant communication overhead (Sync after gradient accumaltion is done)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        if (iter_num + 1) % gradient_accumulation_steps == 0: # Update model after  step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            step_count += 1\n",
        "\n",
        "            # learning rate scheduler\n",
        "            lr = learning_rate - ((learning_rate - 1e-5)/max_iters)*(iter_num)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "            wandb.log({\"lr\": lr})\n",
        "\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        if iter_num % log_interval == 0:\n",
        "            fabric.print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt:.2f}s\")\n",
        "            wandb.log({\"train_iter\": iter_num, \"train_Iter_loss\": loss.item()})\n",
        "\n",
        "       # Saving Adapter weights at the end of epoch\n",
        "        if (iter_num + 1) % epoch_size == 0:\n",
        "            print(f\"Saving adapter weights to {out_dir}\")\n",
        "            save_model_checkpoint(fabric, model, os.path.join(out_dir, f\"iter-{int((iter_num+1)/epoch_size):06d}.pth\"))\n",
        "\n",
        "        # Print and Log val loss\n",
        "            val_loss = validate(fabric, model, val_data)\n",
        "            fabric.print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n",
        "            fabric.barrier()\n",
        "            wandb.log({\"val_step\": iter_num, \"val_step_loss\": val_loss})\n",
        "            print('End of epoch ',(iter_num+1)/epoch_size)\n",
        "\n",
        "\n",
        "\n",
        "def generate_response(model, instruction, input=\"\"):\n",
        "    # To run a inference at the end of epoch, not used in current implementation\n",
        "    tokenizer = Tokenizer(\"model/tokenizer.model\")\n",
        "    sample = {\"instruction\": instruction, \"input\": input}\n",
        "    prompt = generate_prompt(sample)\n",
        "    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n",
        "\n",
        "    output = generate(\n",
        "        model,\n",
        "        idx=encoded,\n",
        "        max_seq_length=max_seq_length,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "    output = tokenizer.decode(output)\n",
        "    return output # output.split(\"### Response:\")[1].strip()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "\n",
        "    if len(val_data) == val_data_len : # To adjust eval_iters for val dataset\n",
        "        eval_iters =  val_data_len // micro_batch_size  // devices\n",
        "    else :# To adjust eval_iters for train dataset\n",
        "        eval_iters =  epoch_size // devices\n",
        "\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        input_ids, targets, audio_features = get_batch(fabric, model, val_data)\n",
        "        logits = model(input_ids, audio_features = audio_features)\n",
        "        loss = loss_fn(logits, targets)\n",
        "        losses[k] = loss.item()\n",
        "    val_loss = losses.mean()\n",
        "\n",
        "    # produce an example:\n",
        "    # instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n",
        "    # output = generate_response(model, instruction)\n",
        "    # fabric.print(instruction)\n",
        "    # fabric.print(output)\n",
        "\n",
        "    model.train()\n",
        "    return val_loss.item()\n",
        "\n",
        "def loss_fn(logits, targets):\n",
        "    # shift the targets such that output n predicts token n+1\n",
        "    logits = logits[..., :-1, :].contiguous()\n",
        "    targets = targets[..., 1:].contiguous()\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_batch(fabric: L.Fabric, model ,data: list):\n",
        "    ix = torch.randint(len(data), (micro_batch_size,))\n",
        "    input_ids = [data[i][\"input_ids\"][:max_input_length].type(torch.int64) for i in ix]\n",
        "    labels = [data[i][\"labels_with_masked_input\"][:max_input_length].type(torch.int64) for i in ix]\n",
        "    audio_features = [data[i][\"audio_features\"].type(model.dtype) for i in ix]\n",
        "\n",
        "\n",
        "    max_len = max(len(s) for s in input_ids)\n",
        "\n",
        "    def pad_right(x, pad_id):\n",
        "        # pad right based on the longest sequence\n",
        "        n = max_len - len(x)\n",
        "        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n",
        "\n",
        "    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n",
        "    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n",
        "    af = torch.cat([x for x in audio_features], dim =0)\n",
        "\n",
        "    x, y , af  = fabric.to_device((x.pin_memory(), y.pin_memory(), af.pin_memory()))\n",
        "\n",
        "    return x, y , af\n",
        "\n",
        "\n",
        "\n",
        "def save_model_checkpoint(fabric, model, file_path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    if isinstance(fabric.strategy, DeepSpeedStrategy):\n",
        "        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n",
        "\n",
        "        tmp_path = file_path.with_suffix(\".tmp\")\n",
        "        fabric.save(tmp_path, {\"model\": model})\n",
        "        fabric.barrier()\n",
        "        if fabric.global_rank == 0:\n",
        "\n",
        "            state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_path)\n",
        "            state_dict = adapter_state_from_state_dict(state_dict)\n",
        "            torch.save(state_dict, file_path)\n",
        "            shutil.rmtree(tmp_path)\n",
        "    else:\n",
        "        state_dict = adapter_state_from_state_dict(model.state_dict())\n",
        "        if fabric.global_rank == 0:\n",
        "            torch.save(state_dict, file_path)\n",
        "        fabric.barrier()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment this line if you see an error: \"Expected is_sm80 to be true, but got false\"\n",
        "    # torch.backends.cuda.enable_flash_sdp(False)\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "eSEQG3qAAh5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference/WL-M.py to run inference"
      ],
      "metadata": {
        "id": "ZzrmuZ41v2Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import argparse\n",
        "import tqdm\n",
        "from evaluate import load\n",
        "\n",
        "wd = Path(__file__).parent.parent.resolve() # does not work as jupyter notebook\n",
        "sys.path.append(str(wd))\n",
        "\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import torch\n",
        "import whisper_openAI.whisper as whisper\n",
        "\n",
        "from lit_llama.WL_M import LLaMA, LLaMAConfig, mark_only_adapter_as_trainable, adapter_state_from_state_dict\n",
        "from lit_llama.tokenizer import Tokenizer\n",
        "from lightning.fabric.strategies import DeepSpeedStrategy\n",
        "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
        "from generate.generate_for_WL import generate\n",
        "wer = load(\"wer\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str, help='Path to the directory containing the Adapter checkpoints')\n",
        "parser.add_argument('--save_dir', default= '/ibex/user/radhaks/LLMs/LLaMA_7B/LLAMA_EMNLP_DeepSpeed/predictions/wers',type=str, help='directory to save predictions as JSON files')\n",
        "parser.add_argument('--data_path',type=str, help='Path to dataset to run inference on')\n",
        "parser.add_argument('--alpaca_path',type=str, help='Path to the Alpaca checkpoints')\n",
        "parser.add_argument('--tokenizer_path',type=str, help='Path to the tokenizer')\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "save_dir = args.save_dir\n",
        "root_path = args.root\n",
        "data_path = args.data_path\n",
        "pretrained_path = args.pretrained_path\n",
        "tokenizer_path = args.tokenizer_path\n",
        "\n",
        "files = os.listdir(root_path)\n",
        "files.sort() # files contains all the adapter checkpoints\n",
        "\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "quantize  = None\n",
        "pretrained_path: Path = Path(pretrained_path)\n",
        "tokenizer_path: Path = Path(tokenizer_path)\n",
        "tokenizer = Tokenizer(tokenizer_path)\n",
        "data   = torch.load(data_path,map_location=torch.device('cpu'))\n",
        "\n",
        "assert pretrained_path.is_file()\n",
        "assert tokenizer_path.is_file()\n",
        "\n",
        "\n",
        "fabric = L.Fabric(devices=1)\n",
        "dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
        "\n",
        "\n",
        "print(\"Loading model ...\", file=sys.stderr)\n",
        "\n",
        "# Loading the Alpaca Checkpoint\n",
        "checkpoint = torch.load(pretrained_path)\n",
        "print('loaded LLaMA checkpoint')\n",
        "\n",
        "# Loading the model\n",
        "config = LLaMAConfig(block_size=2048)\n",
        "with fabric.init_module():\n",
        "    model = LLaMA(config)\n",
        "\n",
        "# Adding relevant weigths from the Whisper Decoder to Alpaca State_dict\n",
        "(_, w_ck_pt) = whisper.load_model(\"large-v2\",device='cpu')\n",
        "print('loaded Whisper checkpoint')\n",
        "for n, p in model.named_parameters():\n",
        "    if 'whisper' in n :\n",
        "        #Alapca weight naming example :transformer.h.2.attn.whisper_value.weight\n",
        "        layer = n.split('.')[2]\n",
        "        suffix = n.split('.')[-1]\n",
        "        kv = n.split('.')[4].split('_')[-1]\n",
        "        #Whisper weight naming example :decoder.blocks.3.cross_attn.key.weight\n",
        "        w_key = f'decoder.blocks.{layer}.cross_attn.{kv}.{suffix}'\n",
        "        checkpoint[n] = w_ck_pt['model_state_dict'][w_key].cpu()\n",
        "\n",
        "with fabric.init_module():\n",
        "    # strict=False because missing keys due to adapter weights not containted in state dict\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "print('eveything except llama model loaded')\n",
        "\n",
        "\n",
        "def result(adapter_path,model):\n",
        "# To run inference on one Adapter Checkpoint\n",
        "\n",
        "    t0 = time.time()\n",
        "    adapter_checkpoint = torch.load(adapter_path)\n",
        "    print('loaded Adapter checkpoint')\n",
        "    with fabric.init_module():\n",
        "        model.load_state_dict(adapter_checkpoint, strict=False)\n",
        "    model.to(dtype)\n",
        "\n",
        "    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
        "\n",
        "    model.eval()\n",
        "    model = fabric.setup_module(model)\n",
        "\n",
        "\n",
        "    c = 0\n",
        "    return_dict = {}\n",
        "    pr = []\n",
        "    gt = []\n",
        "    to_json = []\n",
        "\n",
        "    for datapoint in tqdm.tqdm(data):\n",
        "        encoded = datapoint['input_ids_no_response'].to(model.device)\n",
        "        audio_features = datapoint['audio_features'].to(model.device).to(dtype)\n",
        "        ground_truth =  datapoint['ground_truth']\n",
        "\n",
        "        y = generate(model =model, idx = encoded, max_new_tokens =150,max_seq_length=2048,temperature=0.2, top_k=1, eos_id=tokenizer.eos_id, audio_features= audio_features)\n",
        "\n",
        "        model.reset_cache()\n",
        "        output = tokenizer.decode(y)\n",
        "        inf = output[len(tokenizer.decode(encoded)):].split('\\n')[0].strip() # Removing the prompt from the model output\n",
        "        ref = ground_truth.strip()\n",
        "\n",
        "        if inf == ref: # we increase the count if the inference compleately matches the ground\n",
        "            c = c + 1\n",
        "        pr.append(inf)\n",
        "        gt.append(ref)\n",
        "        to_json.append({'inference':inf, 'ground_truth':ref})\n",
        "\n",
        "    print(f'For {adapter_path}')\n",
        "    return_dict['adapter_path']=adapter_path\n",
        "    wer_ = wer.compute(predictions=pr, references=gt)\n",
        "    print(f'WER is {wer_}')\n",
        "    return_dict['WER']=wer_\n",
        "    print(f'Ground truth matches is {c}/{len(data)}')\n",
        "    to_json.append({'wer':wer_, 'gtms':f'{c}/{len(data)}'})\n",
        "    return_dict['gtms']=c/len(data)\n",
        "\n",
        "    # Saving the results to a JSON file here\n",
        "    with open(os.path.join(save_dir,adapter_path.split('/')[-2]+adapter_path.split('/')[-1]+'.json'),'w') as f:\n",
        "        f.write(json.dumps(to_json , indent = 4,ensure_ascii=False))\n",
        "    print(os.path.join(save_dir,adapter_path.split('/')[-2]+'.json'))\n",
        "\n",
        "    print('the post string normalization wer is')\n",
        "    x = 0\n",
        "    for i in range(len(pr)):\n",
        "        pr[i] = pr[i].lower().replace('.','').replace(',','').replace('-','').replace('?','').replace(\"'\",'')\n",
        "        gt[i] = gt[i].lower().replace('.','').replace(',','').replace('-','').replace('?','').replace(\"'\",'')\n",
        "        if pr[i] == gt[i]:\n",
        "            x = x+1\n",
        "    post_wer =wer.compute(predictions=pr, references=gt)\n",
        "    print('WER',post_wer)\n",
        "    return_dict['post_ST_wer']=post_wer\n",
        "    print(x,'/',len(pr))\n",
        "    return_dict['post_gtms']=x/len(pr)\n",
        "    print('*********************')\n",
        "    return return_dict\n",
        "\n",
        "# In case you want to log the metrics in WandB\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"WHISPERing LLaMA\",\n",
        "    name=root_path,\n",
        ")\n",
        "\n",
        "# Iterating through the Adapter Checkpoints and running inference for each checkpoint\n",
        "for i in files:\n",
        "    adapter_path = os.path.join(root_path,i)\n",
        "    try:\n",
        "        result_dict = result(adapter_path,model)\n",
        "        wer_percent = result_dict['WER']*100\n",
        "        wer_percent_post = result_dict['post_ST_wer']*100\n",
        "\n",
        "        gt_percent = result_dict['gtms']*100\n",
        "        gt_percent_post = result_dict['post_gtms']*100\n",
        "        wandb.log({'epoch':i,\n",
        "                'WER':wer_percent,\n",
        "                \"WER_post\":wer_percent_post,\n",
        "                \"GTM\":gt_percent,\n",
        "                \"GTM_post\":gt_percent_post})\n",
        "    except:\n",
        "        print('skippin',adapter_path)"
      ],
      "metadata": {
        "id": "M6yKG7Z4AkUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To train the model\n",
        "!python training/WL-S.py --lr 1e-3 --d 1 --pretrained_path 'weights/alpaca.pth' --tokenizer_path 'weights/tokenizer.model' --data ''\n",
        "\n",
        "# Refer to https://github.com/Srijith-rkr/Whispering-LLaMA/tree/main/data_preparation to make your own dataset"
      ],
      "metadata": {
        "id": "t16G7TlNAuIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}